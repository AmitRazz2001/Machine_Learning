{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''Natural language processing (NLP) is a branch of artificial intelligence that helps computers understand,\n",
    "         interpret and manipulate human language. NLP draws from many disciplines, including computer science and computational linguistics,\n",
    "         in its pursuit to fill the gap between human communication and computer understanding.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural language processing (NLP) is a branch of artificial intelligence that helps computers understand,\\n         interpret and manipulate human language.', 'NLP draws from many disciplines, including computer science and computational linguistics,\\n         in its pursuit to fill the gap between human communication and computer understanding.']\n"
     ]
    }
   ],
   "source": [
    "# sentence tokenization\n",
    "sentence = nltk.sent_tokenize(text)\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['My', 'name', 'is', 'Amit', 'Razz.I', \"'m\", 'a', 'software', 'developer.I', \"'m\", 'also', 'learn', 'data', 'science', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk import sent_tokenize,word_tokenize\n",
    "\n",
    "paragraph = \"My name is Amit Razz.I'm a software developer.I'm also learn data science.\"\n",
    "# word tokenization\n",
    "\n",
    "word = word_tokenize(paragraph)\n",
    "print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learn', 'developer.I', 'a', 'name', 'science', 'My', '.', 'is', 'Razz.I', 'also', \"'m\", 'data', 'software', 'Amit'}\n"
     ]
    }
   ],
   "source": [
    "print(set(word_tokenize(paragraph)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_word = stopwords.words(\"english\")\n",
    "stop_word\n",
    "print(len(stop_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['My', 'name', 'is', 'Amit', 'Razz.I', \"'m\", 'a', 'software', 'developer.I', \"'m\", 'also', 'learn', 'data', 'science', '.']\n"
     ]
    }
   ],
   "source": [
    "print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['My', 'name', 'Amit', 'Razz.I', \"'m\", 'software', 'developer.I', \"'m\", 'also', 'learn', 'data', 'science', '.']\n"
     ]
    }
   ],
   "source": [
    "filtered = []\n",
    "\n",
    "for w in word:\n",
    "    if w not in stop_word:\n",
    "        filtered.append(w)\n",
    "        \n",
    "print(filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"'m\",\n",
       " '.',\n",
       " 'Amit',\n",
       " 'My',\n",
       " 'Razz.I',\n",
       " 'also',\n",
       " 'data',\n",
       " 'developer.I',\n",
       " 'learn',\n",
       " 'name',\n",
       " 'science',\n",
       " 'software'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# or using list comprehension:\n",
    "\n",
    "filtered = [w for w in word if w not in stop_word]\n",
    "set(filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural language processing (NLP) is a branch of artificial intelligence that helps computers understand,\\n         interpret and manipulate human language.', 'NLP draws from many disciplines, including computer science and computational linguistics,\\n         in its pursuit to fill the gap between human communication and computer understanding.']\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "import pandas as pd\n",
    "\n",
    "text = '''Natural language processing (NLP) is a branch of artificial intelligence that helps computers understand,\n",
    "         interpret and manipulate human language. NLP draws from many disciplines, including computer science and computational linguistics,\n",
    "         in its pursuit to fill the gap between human communication and computer understanding.'''\n",
    "\n",
    "# sentence tokenization\n",
    "sentence = nltk.sent_tokenize(text)\n",
    "print(sentence)\n",
    "print(len(sentence))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['natur', 'languag', 'process', '(', 'nlp', ')', 'branch', 'artifici', 'intellig', 'help', 'comput', 'understand', ',', 'interpret', 'manipul', 'human', 'languag', '.']\n",
      "['nlp', 'draw', 'mani', 'disciplin', ',', 'includ', 'comput', 'scienc', 'comput', 'linguist', ',', 'pursuit', 'fill', 'gap', 'human', 'commun', 'comput', 'understand', '.']\n"
     ]
    }
   ],
   "source": [
    "# Stemming\n",
    "stemmer = PorterStemmer()\n",
    "for i in range(len(sentence)):\n",
    "    words = word_tokenize(sentence[i])\n",
    "    words = [stemmer.stem(word) for word in words if word not in set(stopwords.words(\"english\"))]\n",
    "    sentence[i] = ' '.join(words)\n",
    "    df = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
